{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf4944ad-ac03-4200-a224-4a5e9b06911c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-17T14:51:39.320900Z",
     "iopub.status.busy": "2022-09-17T14:51:39.320567Z",
     "iopub.status.idle": "2022-09-17T14:51:41.316698Z",
     "shell.execute_reply": "2022-09-17T14:51:41.316205Z",
     "shell.execute_reply.started": "2022-09-17T14:51:39.320833Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import tables\n",
    "import pickle\n",
    "import copy\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0ed3cfd-38e6-4455-87f2-5ec1c0c95493",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-17T14:51:41.317975Z",
     "iopub.status.busy": "2022-09-17T14:51:41.317757Z",
     "iopub.status.idle": "2022-09-17T14:51:41.321202Z",
     "shell.execute_reply": "2022-09-17T14:51:41.320775Z",
     "shell.execute_reply.started": "2022-09-17T14:51:41.317956Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9d1e5b-476d-4bcb-a032-273d887369da",
   "metadata": {},
   "source": [
    "# load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c39933d-8424-47c9-b416-243088dfca4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-17T14:51:41.321955Z",
     "iopub.status.busy": "2022-09-17T14:51:41.321823Z",
     "iopub.status.idle": "2022-09-17T14:51:41.326296Z",
     "shell.execute_reply": "2022-09-17T14:51:41.325878Z",
     "shell.execute_reply.started": "2022-09-17T14:51:41.321939Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def identify_constant_columns(data):\n",
    "\n",
    "    constant_cols_list = []\n",
    "\n",
    "    for index in range(data.shape[1]):\n",
    "\n",
    "        col = data[:, index]\n",
    "        is_unique = len(np.unique(col)) == 1\n",
    "\n",
    "        if is_unique:\n",
    "            constant_cols_list.append(index)\n",
    "            \n",
    "    return constant_cols_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74dfd2ea-2723-4d26-bbc5-d880fbd6ae29",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-17T14:51:41.327470Z",
     "iopub.status.busy": "2022-09-17T14:51:41.327295Z",
     "iopub.status.idle": "2022-09-17T14:51:41.331811Z",
     "shell.execute_reply": "2022-09-17T14:51:41.331399Z",
     "shell.execute_reply.started": "2022-09-17T14:51:41.327452Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def scale_data_and_pca(tr_x, val_x, dataset, pca=True):\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(tr_x)\n",
    "    tr_x = scaler.transform(tr_x).astype(np.float32)\n",
    "    val_x = scaler.transform(val_x).astype(np.float32)\n",
    "    \n",
    "    if pca:\n",
    "        file_name = f\"pca_{dataset}.p\"\n",
    "\n",
    "        if os.path.exists(file_name):\n",
    "            pca = pickle.load(open(file_name, \"rb\"))\n",
    "        else:\n",
    "            pca = PCA(tr_x.shape[1])\n",
    "            pca.fit(tr_x)\n",
    "            pickle.dump(pca, open(file_name, \"wb\"))\n",
    "\n",
    "        tr_x = pca.transform(tr_x)\n",
    "        val_x = pca.transform(val_x)\n",
    "    \n",
    "    return tr_x, val_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1fcc51b-ee76-4482-ac1a-549a04420259",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-17T14:51:41.332497Z",
     "iopub.status.busy": "2022-09-17T14:51:41.332364Z",
     "iopub.status.idle": "2022-09-17T14:51:41.336360Z",
     "shell.execute_reply": "2022-09-17T14:51:41.335944Z",
     "shell.execute_reply.started": "2022-09-17T14:51:41.332481Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def merge_data_with_metadata(train_x, test_x, metadata, index_train, index_test, index_meta_data):\n",
    "    \n",
    "    metadata = pd.DataFrame(np.hstack((np.expand_dims(index_meta_data, 1), metadata)))\n",
    "    \n",
    "    train_x = pd.DataFrame(np.hstack((np.expand_dims(index_train, 1), train_x))).merge(metadata, how=\"inner\", left_on=0, right_on=0)\n",
    "    test_x = pd.DataFrame(np.hstack((np.expand_dims(index_test, 1), test_x))).merge(metadata, how=\"inner\", left_on=0, right_on=0)\n",
    "    \n",
    "    return train_x, test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f7faa33-c6cf-4cb2-8774-a23091b935f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-17T14:51:41.337200Z",
     "iopub.status.busy": "2022-09-17T14:51:41.337070Z",
     "iopub.status.idle": "2022-09-17T14:51:41.340218Z",
     "shell.execute_reply": "2022-09-17T14:51:41.339808Z",
     "shell.execute_reply.started": "2022-09-17T14:51:41.337184Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_metadata():\n",
    "    \n",
    "    metadata = pd.read_csv(\"/kaggle/input/open-problems-multimodal/metadata.csv\")\n",
    "    index = metadata[\"cell_id\"]\n",
    "    \n",
    "    enc_metadata = OneHotEncoder(handle_unknown='ignore')\n",
    "    metadata = enc_metadata.fit_transform(metadata[[\"day\", \"donor\", \"cell_type\"]].values).toarray()\n",
    "    \n",
    "    return index, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "683d20e5-b7cd-40f2-bd68-3fe43356583f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-17T14:51:41.341077Z",
     "iopub.status.busy": "2022-09-17T14:51:41.340815Z",
     "iopub.status.idle": "2022-09-17T14:51:41.346676Z",
     "shell.execute_reply": "2022-09-17T14:51:41.346253Z",
     "shell.execute_reply.started": "2022-09-17T14:51:41.341060Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the paths to load the data\n",
    "\n",
    "def load_dataset(data_name):\n",
    "\n",
    "    dir_path = \"/kaggle/input/open-problems-multimodal/\"\n",
    "\n",
    "    if data_name == \"citeseq\":\n",
    "        train_x_path = os.path.join(dir_path,\"train_cite_inputs.h5\")\n",
    "        train_y_path = os.path.join(dir_path,\"train_cite_targets.h5\")\n",
    "        test_x_path = os.path.join(dir_path,\"test_cite_inputs.h5\")\n",
    "\n",
    "    elif data_name == \"multiome\":\n",
    "        train_x_path = os.path.join(dir_path,\"train_multi_inputs.h5\")\n",
    "        train_y_path = os.path.join(dir_path,\"train_multi_targets.h5\")\n",
    "        test_x_path = os.path.join(dir_path,\"test_multi_inputs.h5\")\n",
    "\n",
    "    else:\n",
    "        raise NameError(f\"{data_name} is not a valid name: choose between 'siteseq' and 'multiome'\")\n",
    "        \n",
    "    train_x = pd.read_hdf(train_x_path)\n",
    "    train_y = pd.read_hdf(train_y_path)\n",
    "    test_x = pd.read_hdf(test_x_path)\n",
    "    \n",
    "    index_train = train_x.index\n",
    "    index_test = test_x.index\n",
    "    \n",
    "    train_x, train_y, test_x = train_x.to_numpy(), train_y.to_numpy(), test_x.to_numpy()\n",
    "    \n",
    "    # some columns in the training sets are constant (zeros), remove them\n",
    "    constant_columns_train = identify_constant_columns(train_x)\n",
    "    train_x = train_x[:, [i for i in range(train_x.shape[1]) if i not in constant_columns_train]]\n",
    "    test_x = test_x[:, [i for i in range(test_x.shape[1]) if i not in constant_columns_train]]\n",
    "    \n",
    "    return train_x, train_y, test_x, index_train, index_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047d1542-8f5c-44ce-ba16-73f2f083bd0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d8408f9-589b-49ea-9ef3-a1004ffb3fbb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-17T14:51:43.496616Z",
     "iopub.status.busy": "2022-09-17T14:51:43.496335Z",
     "iopub.status.idle": "2022-09-17T14:51:43.499281Z",
     "shell.execute_reply": "2022-09-17T14:51:43.498821Z",
     "shell.execute_reply.started": "2022-09-17T14:51:43.496594Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = \"citeseq\"\n",
    "\n",
    "add_metadata = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49235f38-4907-4fcd-8e96-4366db4127a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-17T14:51:44.058325Z",
     "iopub.status.busy": "2022-09-17T14:51:44.058069Z",
     "iopub.status.idle": "2022-09-17T14:53:23.452784Z",
     "shell.execute_reply": "2022-09-17T14:53:23.452252Z",
     "shell.execute_reply.started": "2022-09-17T14:51:44.058304Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_x, train_y, test_x, index_train, index_test = load_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdb4d9d-03d0-426a-8474-40b9adf1dd2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ec97db68-6b12-413f-bba1-f5d6f70d6ef2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-16T22:15:28.128449Z",
     "iopub.status.busy": "2022-09-16T22:15:28.128148Z",
     "iopub.status.idle": "2022-09-16T22:15:32.968868Z",
     "shell.execute_reply": "2022-09-16T22:15:32.968346Z",
     "shell.execute_reply.started": "2022-09-16T22:15:28.128429Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if add_metadata:\n",
    "\n",
    "#     index_meta_data, meta_data = load_metadata()\n",
    "#     train_x, test_x = merge_data_with_metadata(train_x, test_x, meta_data, index_train, index_test, index_meta_data)\n",
    "    \n",
    "#     train_x = np.array(train_x)[:, 1:]\n",
    "#     test_x = np.array(test_x)[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20dc602-566a-4293-b541-e45b2bedc7d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49be00cb-6d16-4347-9250-4f113f9d1400",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "670cd5bc-4de1-4051-9435-d6195d6c5f60",
   "metadata": {},
   "source": [
    "# Pytorch utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "163135f5-b339-48e5-8cd2-8b05cf47a890",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-17T14:56:30.871003Z",
     "iopub.status.busy": "2022-09-17T14:56:30.870634Z",
     "iopub.status.idle": "2022-09-17T14:56:30.879706Z",
     "shell.execute_reply": "2022-09-17T14:56:30.879254Z",
     "shell.execute_reply.started": "2022-09-17T14:56:30.870980Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# training loop for each epoch\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer, device):\n",
    "    \n",
    "    num_batches = len(dataloader)\n",
    "    model.train()\n",
    "    loss = 0\n",
    "    corr = 0\n",
    "    \n",
    "    for data_dic in dataloader:\n",
    "        X, y = data_dic['x'].to(device), data_dic['y'].to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        train_loss = loss_fn(pred, y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "            \n",
    "        loss += train_loss.item()\n",
    "        corr += np.mean([pearsonr(pred[i].cpu().detach().numpy(), y[i].cpu().detach().numpy())[0] for i in range(pred.shape[0])])\n",
    "    \n",
    "    loss /= num_batches\n",
    "    corr /= num_batches\n",
    "    \n",
    "    return loss, corr\n",
    "\n",
    "\n",
    "# validation loop for each epoch\n",
    "def val(dataloader, model, loss_fn, device):\n",
    "    \n",
    "    pred_val_list = []\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    corr = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data_dic in dataloader:\n",
    "            X, y = data_dic['x'].to(device), data_dic['y'].to(device)\n",
    "            \n",
    "            pred = model(X)\n",
    "            pred_val_list.append(pred.cpu().detach().numpy())\n",
    "            val_loss = loss_fn(pred, y)\n",
    "                \n",
    "            loss += val_loss.item()    \n",
    "            corr += np.mean([pearsonr(pred[i].cpu().detach().numpy(), y[i].cpu().detach().numpy())[0] for i in range(pred.shape[0])])\n",
    "            \n",
    "    loss /= num_batches\n",
    "    corr /= num_batches\n",
    "    \n",
    "    return loss, corr, pred_val_list\n",
    "\n",
    "# make predictions on the test set (dataloader is only made of x)\n",
    "\n",
    "def test(dataloader, model, loss_fn, device):\n",
    "    \n",
    "    pred_list = []\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data_dic in dataloader:\n",
    "            \n",
    "            X = data_dic['x'].to(device)\n",
    "            pred = model(X)\n",
    "            pred_list.append(pred)\n",
    "    \n",
    "    return pred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25b5cbc8-1b5f-4e89-a3eb-1524c279bf81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-17T14:56:31.067216Z",
     "iopub.status.busy": "2022-09-17T14:56:31.066974Z",
     "iopub.status.idle": "2022-09-17T14:56:31.072617Z",
     "shell.execute_reply": "2022-09-17T14:56:31.072181Z",
     "shell.execute_reply.started": "2022-09-17T14:56:31.067196Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dataset class that loads the data and prepare it for the pytorch dataloader\n",
    "\n",
    "class CompetitionDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data_tuple, mode='train'):\n",
    "        self.mode = mode\n",
    "        \n",
    "        if self.mode == \"train\":\n",
    "            assert len(data_tuple) == 2, \"`data_tuple` should have lenght 2\"\n",
    "            data_x, data_y = data_tuple\n",
    "        elif self.mode == \"test\":\n",
    "            assert len(data_tuple) == 1, \"`data_tuple` should have length 1\"\n",
    "            data_x = data_tuple[0]\n",
    "        else:\n",
    "            raise NameError(f\"{self.mode} is not a valid mode: choose between 'train' and 'test'\")\n",
    "\n",
    "        self.filenames = dict()\n",
    "        self.filenames['x'] = data_x\n",
    "        \n",
    "        if self.mode == \"train\":\n",
    "            self.filenames['y'] = data_y\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch = dict()\n",
    "        \n",
    "        batch['x'] = torch.from_numpy(self.filenames['x'][index])\n",
    "        \n",
    "        if self.mode == \"train\":\n",
    "            batch['y'] = torch.from_numpy(self.filenames['y'][index])\n",
    "        \n",
    "        return batch\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames['x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "68937ca6-67cf-479d-b9eb-14ae28e70f8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-17T14:56:31.279870Z",
     "iopub.status.busy": "2022-09-17T14:56:31.279556Z",
     "iopub.status.idle": "2022-09-17T14:56:31.283324Z",
     "shell.execute_reply": "2022-09-17T14:56:31.282882Z",
     "shell.execute_reply.started": "2022-09-17T14:56:31.279851Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pytorch_dataset_and_dataloader(tr_x, tr_y, val_x, val_y,  batch_size = 256, mode = \"train\"):\n",
    "\n",
    "    train_dataset = CompetitionDataset((tr_x, tr_y), mode)\n",
    "    val_dataset = CompetitionDataset((val_x, val_y), mode) # here \"train\" is also used for validation\n",
    "    \n",
    "    train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size, drop_last=False)\n",
    "    val_dataloader = DataLoader(val_dataset, shuffle=True, batch_size=batch_size, drop_last=False)\n",
    "    \n",
    "    return train_dataloader, val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e2650a-f4ae-4599-a74b-c880ed61e50d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ba34393-bbb4-497b-9b3d-2e895ad41ca9",
   "metadata": {},
   "source": [
    "# Pytorch simple MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d64bc64c-1a4b-4385-ad54-671f6b6ee650",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-17T14:56:32.043812Z",
     "iopub.status.busy": "2022-09-17T14:56:32.043539Z",
     "iopub.status.idle": "2022-09-17T14:56:32.054622Z",
     "shell.execute_reply": "2022-09-17T14:56:32.054185Z",
     "shell.execute_reply.started": "2022-09-17T14:56:32.043790Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, n_shared_hidden_layers, shared_hidden_size_list, n_non_shared_hidden_layers, non_shared_hidden_size_list, \n",
    "                 dropout, output_size, use_multi_head=False):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        \n",
    "        assert len(shared_hidden_size_list) == n_shared_hidden_layers, f\"`hidden_size_list` should have length {n_shared_hidden_layers}\"\n",
    "        assert len(non_shared_hidden_size_list) == n_non_shared_hidden_layers, f\"`hidden_size_list` should have length {n_non_shared_hidden_layers}\"\n",
    "        \n",
    "        self.n_shared_hidden_layers = n_shared_hidden_layers\n",
    "        self.shared_hidden_size = shared_hidden_size_list\n",
    "        self.n_non_shared_hidden_layers = n_non_shared_hidden_layers\n",
    "        self.non_shared_hidden_size = non_shared_hidden_size_list\n",
    "        self.dropout = dropout\n",
    "        self.output_size = output_size\n",
    "        self.use_multi_head = use_multi_head\n",
    "        \n",
    "        # hidden layers\n",
    "        self.shared_hidden_layers_list = nn.ModuleList()\n",
    "        for l in range(self.n_shared_hidden_layers):\n",
    "            if l == 0:\n",
    "                self.shared_hidden_layers_list.append(nn.LazyLinear(self.shared_hidden_size[l]))\n",
    "            else:\n",
    "                self.shared_hidden_layers_list.append(nn.Linear(self.shared_hidden_size[l-1], self.shared_hidden_size[l]))\n",
    "            self.shared_hidden_layers_list.append(nn.ReLU())\n",
    "            self.shared_hidden_layers_list.append(nn.Dropout(self.dropout))\n",
    "            self.shared_hidden_layers_list.append(nn.BatchNorm1d(self.shared_hidden_size[l]))\n",
    "            \n",
    "        self.shared_hidden_layers_list = nn.Sequential(*self.shared_hidden_layers_list)\n",
    "        \n",
    "        if self.use_multi_head:\n",
    "            self.non_shared_hidden_layers = nn.ModuleList(nn.ModuleList() for _ in range(self.output_size))\n",
    "            for i in range(self.output_size):\n",
    "                for l in range(self.n_non_shared_hidden_layers):\n",
    "                    if l == 0:\n",
    "                        self.non_shared_hidden_layers[i].append(nn.Linear(self.shared_hidden_size[-1], self.non_shared_hidden_size[l]))\n",
    "                    else:\n",
    "                        self.non_shared_hidden_layers[i].append(nn.Linear(self.non_shared_hidden_size[l-1], self.non_shared_hidden_size[l]))\n",
    "                    self.non_shared_hidden_layers[i].append(nn.ReLU())\n",
    "                    self.non_shared_hidden_layers[i].append(nn.Dropout(self.dropout))\n",
    "                    self.non_shared_hidden_layers[i].append(nn.BatchNorm1d(self.non_shared_hidden_size[l]))\n",
    "                self.non_shared_hidden_layers[i].append(nn.Linear(self.non_shared_hidden_size[-1], 1))\n",
    "                \n",
    "                self.non_shared_hidden_layers[i] = nn.Sequential(*self.non_shared_hidden_layers[i])\n",
    "        \n",
    "        self.output_layer = nn.Linear(self.shared_hidden_size[-1], output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        shared_x = self.shared_hidden_layers_list(x)\n",
    "            \n",
    "        if self.use_multi_head:\n",
    "            output = torch.empty(size=(shared_x.shape[0], self.output_size)).to(\"cuda\")\n",
    "            for i in range(self.output_size):\n",
    "                not_shared_x = shared_x\n",
    "                not_shared_x = self.non_shared_hidden_layers[i](not_shared_x)\n",
    "                output[:, i] = torch.squeeze(not_shared_x)\n",
    "        else:\n",
    "            output = self.output_layer(shared_x)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c88c17f3-4029-4485-86c7-ebbdc8759cb3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-17T14:56:32.398004Z",
     "iopub.status.busy": "2022-09-17T14:56:32.397737Z",
     "iopub.status.idle": "2022-09-17T14:56:32.400638Z",
     "shell.execute_reply": "2022-09-17T14:56:32.400203Z",
     "shell.execute_reply.started": "2022-09-17T14:56:32.397982Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pca = True\n",
    "n_components_to_keep = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f96102ba-2079-4564-a585-246bc6cf2590",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-17T14:56:34.774116Z",
     "iopub.status.busy": "2022-09-17T14:56:34.773830Z",
     "iopub.status.idle": "2022-09-17T14:56:34.777358Z",
     "shell.execute_reply": "2022-09-17T14:56:34.776915Z",
     "shell.execute_reply.started": "2022-09-17T14:56:34.774095Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# network parameters\n",
    "\n",
    "n_shared_hidden_layers = 2\n",
    "n_non_shared_hidden_layers = 2\n",
    "\n",
    "dropout = 0.5\n",
    "\n",
    "shared_hidden_size_list = [1024, 1024] \n",
    "non_shared_hidden_size_list = [1024, 512] \n",
    "\n",
    "output_size = train_y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c74c04a-dac2-4aee-be3b-f169b8bfcc33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-17T14:56:36.581114Z",
     "iopub.status.busy": "2022-09-17T14:56:36.580831Z",
     "iopub.status.idle": "2022-09-17T14:56:36.583729Z",
     "shell.execute_reply": "2022-09-17T14:56:36.583281Z",
     "shell.execute_reply.started": "2022-09-17T14:56:36.581092Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "use_multi_head = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2adc00d9-7376-4d0b-857b-66d584ca8698",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-17T14:56:37.460435Z",
     "iopub.status.busy": "2022-09-17T14:56:37.460157Z",
     "iopub.status.idle": "2022-09-17T14:56:37.463072Z",
     "shell.execute_reply": "2022-09-17T14:56:37.462634Z",
     "shell.execute_reply.started": "2022-09-17T14:56:37.460414Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "batch_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "46db79fa-c568-4afd-90e9-1af19ff90a8f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-17T14:56:40.354370Z",
     "iopub.status.busy": "2022-09-17T14:56:40.354079Z",
     "iopub.status.idle": "2022-09-17T14:56:40.356997Z",
     "shell.execute_reply": "2022-09-17T14:56:40.356523Z",
     "shell.execute_reply.started": "2022-09-17T14:56:40.354348Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_cv_fold = 5\n",
    "n_epoch = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5366ec45-8a73-47da-84fa-d87cd10e20bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-17T12:54:07.491185Z",
     "iopub.status.busy": "2022-09-17T12:54:07.490929Z",
     "iopub.status.idle": "2022-09-17T13:11:11.673862Z",
     "shell.execute_reply": "2022-09-17T13:11:11.673362Z",
     "shell.execute_reply.started": "2022-09-17T12:54:07.491162Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing for fold number 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/saturn/lib/python3.9/site-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTER LOOP epoch 0\n",
      "training loss: 9.193361662529611, corr train: 0.5281938819416625, validation_loss: 3.1056170974458968, corr val: 0.8756526305186643\n",
      "OUTER LOOP epoch 1\n",
      "training loss: 3.154270174267056, corr train: 0.866099737489893, validation_loss: 2.4696242128099715, corr val: 0.897057650242531\n",
      "OUTER LOOP epoch 2\n",
      "training loss: 2.8757982382903227, corr train: 0.878629707941741, validation_loss: 2.424633707318987, corr val: 0.8987493494876678\n",
      "OUTER LOOP epoch 3\n",
      "training loss: 2.7566060633272738, corr train: 0.8836650708735669, validation_loss: 2.3993845752307346, corr val: 0.899363202799381\n",
      "OUTER LOOP epoch 4\n",
      "training loss: 2.6669177386137815, corr train: 0.8870770573189948, validation_loss: 2.398426490170615, corr val: 0.8996271371875565\n",
      "OUTER LOOP epoch 5\n",
      "training loss: 2.603389888196378, corr train: 0.8894401262301211, validation_loss: 2.365670212677547, corr val: 0.9003941112505461\n",
      "OUTER LOOP epoch 6\n",
      "training loss: 2.558800740284963, corr train: 0.8911160914549549, validation_loss: 2.3703789625849043, corr val: 0.9006200014746515\n",
      "OUTER LOOP epoch 7\n",
      "training loss: 2.5243830659368016, corr train: 0.8923624301414759, validation_loss: 2.368856055395944, corr val: 0.9009188217572116\n",
      "OUTER LOOP epoch 8\n",
      "training loss: 2.4947210625485257, corr train: 0.8933648277340094, validation_loss: 2.3683756419590543, corr val: 0.9008781148882627\n",
      "OUTER LOOP epoch 9\n",
      "training loss: 2.4692635793943665, corr train: 0.8942063538873605, validation_loss: 2.346343159675598, corr val: 0.901189334050776\n",
      "OUTER LOOP epoch 10\n",
      "training loss: 2.4508304209322542, corr train: 0.8949388157098811, validation_loss: 2.351197191647121, corr val: 0.9014590028265659\n",
      "OUTER LOOP epoch 11\n",
      "training loss: 2.4308344866778397, corr train: 0.8955774748426785, validation_loss: 2.3630951642990112, corr val: 0.901288045985737\n",
      "OUTER LOOP epoch 12\n",
      "training loss: 2.411802968463382, corr train: 0.8960822191276029, validation_loss: 2.355092006070273, corr val: 0.9013946721692713\n",
      "OUTER LOOP epoch 13\n",
      "training loss: 2.3986423122990237, corr train: 0.8966557646726312, validation_loss: 2.3436374919755116, corr val: 0.9015699462436909\n",
      "OUTER LOOP epoch 14\n",
      "training loss: 2.3851273639782056, corr train: 0.8969700092818523, validation_loss: 2.3719242811203003, corr val: 0.9016302538551105\n",
      "OUTER LOOP epoch 15\n",
      "training loss: 2.371495904149236, corr train: 0.8974997787352256, validation_loss: 2.3773675305502757, corr val: 0.9017815054661247\n",
      "OUTER LOOP epoch 16\n",
      "training loss: 2.3628606688868894, corr train: 0.8978134247038777, validation_loss: 2.3439431956836154, corr val: 0.901659585723058\n",
      "OUTER LOOP epoch 17\n",
      "training loss: 2.354384776708242, corr train: 0.8980219692986682, validation_loss: 2.3707884890692577, corr val: 0.9017489090195223\n",
      "OUTER LOOP epoch 18\n",
      "training loss: 2.341354803996043, corr train: 0.8984659737448701, validation_loss: 2.3541807447160994, corr val: 0.9018031019359688\n",
      "OUTER LOOP epoch 19\n",
      "training loss: 2.3315897030873343, corr train: 0.8987130924766557, validation_loss: 2.3721509660993303, corr val: 0.9018957266553941\n"
     ]
    }
   ],
   "source": [
    "# cross validation\n",
    "\n",
    "fold_number = 0\n",
    "\n",
    "for train_index_outer, val_index_outer in KFold(n_cv_fold, shuffle=True, random_state=0).split(train_x):\n",
    "    print(f\"computing for fold number {fold_number+1}\")\n",
    "    \n",
    "    tr_x, tr_y = train_x[train_index_outer], train_y[train_index_outer]\n",
    "    val_x, val_y = train_x[val_index_outer], train_y[val_index_outer]\n",
    "    \n",
    "    tr_x, val_x =  scale_data_and_pca(tr_x, val_x, dataset, pca)\n",
    "    tr_x = tr_x[:, :n_components_to_keep]\n",
    "    val_x = val_x[:, :n_components_to_keep]\n",
    "    \n",
    "    train_dataloader, val_dataloader = pytorch_dataset_and_dataloader(tr_x, tr_y, val_x, val_y, batch_size, \"train\")\n",
    "    \n",
    "    # instantiate a newly initialized model and a new optimize (adam aggregates gradients so it needs to be reset)\n",
    "    model = NeuralNetwork(n_shared_hidden_layers, shared_hidden_size_list, n_non_shared_hidden_layers, \n",
    "                          non_shared_hidden_size_list, dropout, output_size, use_multi_head).to(device)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in range(n_epoch):\n",
    "        \n",
    "        # train for one full epoch and gather training_loss\n",
    "        loss_train, corr_train = train(train_dataloader, model, loss_fn, optimizer, device)\n",
    "        # compute the validation loss after the epoch\n",
    "        loss_val, corr_val, pred_val_list = val(val_dataloader, model, loss_fn, device)\n",
    "        \n",
    "        print(f\"OUTER LOOP epoch {epoch}\")\n",
    "        print(f\"training loss: {loss_train}, corr train: {corr_train}, validation_loss: {loss_val}, corr val: {corr_val}\")\n",
    "        \n",
    "    break\n",
    "\n",
    "    fold_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6f8f846d-d843-4bfc-a8f9-3065794b28c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-17T12:53:42.012015Z",
     "iopub.status.busy": "2022-09-17T12:53:42.011720Z",
     "iopub.status.idle": "2022-09-17T12:53:42.060576Z",
     "shell.execute_reply": "2022-09-17T12:53:42.060116Z",
     "shell.execute_reply.started": "2022-09-17T12:53:42.011992Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745b3b1d-0f47-45b0-8b28-9e07e208cca2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7cd832-0340-4448-9bfb-bc194b06dcc4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-17T14:56:47.011513Z",
     "iopub.status.busy": "2022-09-17T14:56:47.011018Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing for fold number 1\n"
     ]
    }
   ],
   "source": [
    "# cross validation\n",
    "\n",
    "fold_number = 0\n",
    "\n",
    "for train_index_outer, val_index_outer in KFold(n_cv_fold, shuffle=True, random_state=0).split(train_x):\n",
    "    print(f\"computing for fold number {fold_number+1}\")\n",
    "    \n",
    "    tr_x, tr_y = train_x[train_index_outer], train_y[train_index_outer]\n",
    "    val_x, val_y = train_x[val_index_outer], train_y[val_index_outer]\n",
    "    \n",
    "    tr_x, val_x =  scale_data_and_pca(tr_x, val_x, dataset, pca)\n",
    "    tr_x = tr_x[:, :n_components_to_keep]\n",
    "    val_x = val_x[:, :n_components_to_keep]\n",
    "    \n",
    "    train_dataloader, val_dataloader = pytorch_dataset_and_dataloader(tr_x, tr_y, val_x, val_y, batch_size, \"train\")\n",
    "    \n",
    "    # instantiate a newly initialized model and a new optimize (adam aggregates gradients so it needs to be reset)\n",
    "    model = NeuralNetwork(n_shared_hidden_layers, shared_hidden_size_list, n_non_shared_hidden_layers, \n",
    "                          non_shared_hidden_size_list, dropout, output_size, use_multi_head).to(device)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in range(1):\n",
    "        \n",
    "        # train for one full epoch and gather training_loss\n",
    "        loss_train, corr_train = train(train_dataloader, model, loss_fn, optimizer, device)\n",
    "        # compute the validation loss after the epoch\n",
    "        loss_val, corr_val, pred_val_list = val(val_dataloader, model, loss_fn, device)\n",
    "        \n",
    "        print(f\"OUTER LOOP epoch {epoch}\")\n",
    "        print(f\"training loss: {loss_train}, corr train: {corr_train}, validation_loss: {loss_val}, corr val: {corr_val}\")\n",
    "    \n",
    "    pred_train = model(torch.from_numpy(val_x).to(device))\n",
    "    pred_train = pred_train.cpu().detach().numpy()\n",
    "    pred_train = np.hstack((val_x, pred_train))\n",
    "        \n",
    "    for train_index_inner, val_index_inner in KFold(10, shuffle=True, random_state=0).split(pred_train):\n",
    "        \n",
    "        tr_x_inner, tr_y_inner = pred_train[train_index_inner], val_y[train_index_inner]\n",
    "        val_x_inner, val_y_inner = pred_train[val_index_inner], val_y[val_index_inner]\n",
    "        \n",
    "        tr_x, val_x =  scale_data_and_pca(tr_x_inner, val_x_inner, dataset, False)\n",
    "        \n",
    "        train_dataloader, val_dataloader = pytorch_dataset_and_dataloader(tr_x_inner, tr_y_inner, val_x_inner, val_y_inner, batch_size, \"train\")\n",
    "        \n",
    "        model = NeuralNetwork(n_shared_hidden_layers, shared_hidden_size_list, n_non_shared_hidden_layers, \n",
    "                          non_shared_hidden_size_list, dropout, output_size, use_multi_head).to(device)\n",
    "        loss_fn = nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        \n",
    "        for epoch in range(50):\n",
    "        \n",
    "            # train for one full epoch and gather training_loss\n",
    "            loss_train, corr_train = train(train_dataloader, model, loss_fn, optimizer, device)\n",
    "            # compute the validation loss after the epoch\n",
    "            loss_val = val(val_dataloader, model, loss_fn, device)\n",
    "            \n",
    "            print(f\"INNER LOOP epoch {epoch}\")\n",
    "            print(f\"training loss: {loss_train}, corr train: {corr_train}, validation_loss: {loss_val}\")\n",
    "\n",
    "    fold_number += 1\n",
    "        \n",
    "    break\n",
    "\n",
    "    fold_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6aa8ae-d9c1-4f0b-9168-9b5746da2661",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd19fae8-54fb-469c-b328-4e61b45994c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d9a2db-f890-4382-aff7-01188c87a190",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4838a82-d5c6-47b2-aaa1-fd2afebd7829",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf63b82-5218-458a-a7f9-5396b70a2bb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saturn (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
